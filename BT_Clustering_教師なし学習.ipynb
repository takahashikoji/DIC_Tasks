{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 課題 卸売業者 教師なし学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【課題】教師なし学習とは何か記述せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え：　　\n",
    "\n",
    "■教師あり学習との比較   \n",
    "- 教師あり学習 - 人間が付けたラベル（説明変数）によって、教えられた構造を学び取る。入力データとその正解が与えられる。  \n",
    "- 教師なし学習 - 与えられたデータ（特徴量）から規則性を発見して学び取る。入力データのみで正解は与えられない。\n",
    "\n",
    "■どんなときに使うのか  \n",
    "- 正解が与えられない問題や、与えられたデータの背後に存在する本質的な構造を抽出するために教師なし学習が用いられる。未知のデータの特徴を発見したり予測したりする場合に教師なし学習を使う場合が多い。\n",
    "\n",
    "■具体的にどんなものがあるのか  \n",
    "- クラスタリング  \n",
    "階層型クラスタリング\n",
    "非階層型クラスタリング(K-Means)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【課題】必要なライブラリをimport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データ取得"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'Wholesale customers data.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-488bad916ef1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Wholesale customers data.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    703\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    704\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 445\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    447\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    812\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/anaconda3-4.3.0/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1682\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1684\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1686\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: File b'Wholesale customers data.csv' does not exist"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('Wholesale customers data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Information:\n",
    "\n",
    "1)\tFRESH: annual spending (m.u.) on fresh products (Continuous); \n",
    "2)\tMILK: annual spending (m.u.) on milk products (Continuous); \n",
    "3)\tGROCERY: annual spending (m.u.)on grocery products (Continuous); \n",
    "4)\tFROZEN: annual spending (m.u.)on frozen products (Continuous) \n",
    "5)\tDETERGENTS_PAPER: annual spending (m.u.) on detergents and paper products (Continuous) \n",
    "6)\tDELICATESSEN: annual spending (m.u.)on and delicatessen products (Continuous); \n",
    "7)\tCHANNEL: customersâ€™ Channel - Horeca (Hotel/Restaurant/CafÃ©) or Retail channel (Nominal) \n",
    "8)\tREGION: customersâ€™ Region â€“ Lisnon, Oporto or Other (Nominal) \n",
    "Descriptive Statistics: \n",
    "\n",
    "(Minimum, Maximum, Mean, Std. Deviation) \n",
    "FRESH (\t3, 112151, 12000.30, 12647.329) \n",
    "MILK\t(55, 73498, 5796.27, 7380.377) \n",
    "GROCERY\t(3, 92780, 7951.28, 9503.163) \n",
    "FROZEN\t(25, 60869, 3071.93, 4854.673) \n",
    "DETERGENTS_PAPER (3, 40827, 2881.49, 4767.854) \n",
    "DELICATESSEN (3, 47943, 1524.87, 2820.106) \n",
    "\n",
    "REGION\tFrequency \n",
    "Lisbon\t77 \n",
    "Oporto\t47 \n",
    "Other Region\t316 \n",
    "Total\t440 \n",
    "\n",
    "CHANNEL\tFrequency \n",
    "Horeca\t298 \n",
    "Retail\t142 \n",
    "Total\t440  \n",
    "\n",
    "Relevant Papers:\n",
    "\n",
    "Cardoso, Margarida G.M.S. (2013). Logical discriminant models â€“ Chapter 8 in Quantitative Modeling in Marketing and Management Edited by Luiz Moutinho and Kun-Huang Huarng. World Scientific. p. 223-253. ISBN 978-9814407717 \n",
    "\n",
    "Jean-Patrick Baudry, Margarida Cardoso, Gilles Celeux, Maria JosÃ© Amorim, Ana Sousa Ferreira (2012). Enhancing the selection of a model-based clustering with external qualitative variables. RESEARCH REPORT NÂ° 8124, October 2012, Project-Team SELECT. INRIA Saclay - ÃŽle-de-France, Projet select, UniversitÃ© Paris-Sud 11 \n",
    "\n",
    "\n",
    "\n",
    "Citation Request:\n",
    "\n",
    "The data set is originated from a larger database referred on: \n",
    "\n",
    "Abreu, N. (2011). Analise do perfil do cliente Recheio e desenvolvimento de um sistema promocional. Mestrado em Marketing, ISCTE-IUL, Lisbon \n",
    "\n",
    "\n",
    "\n",
    "属性情報：\n",
    "\n",
    "1）フレッシュ：新鮮な製品の年間消費（m.u.）。\n",
    "2）乳製品：乳製品の年間支出（m.u.）。\n",
    "3）食料品：食料品（年間）に対する年間支出（m.u.）。\n",
    "4）冷凍：凍結製品の年間支出（連続）\n",
    "5）DETERGENTS_PAPER：洗剤および紙製品の年間支出（m.u.）（継続的）\n",
    "6）デリカテッセン（Delicatessen）：デリカテッセン製品に対する年間支出（m.u.）。\n",
    "7）チャネル：customersâ€™チャネル - Horeca（ホテル/レストラン/カフェ）またはリテールチャネル（名目）\n",
    "8）地域：customersâ€™地域 - リスボン、ポルトガル、その他（名）\n",
    "記述統計量：\n",
    "\n",
    "（最小、最大、平均、標準偏差）\n",
    "FRESH（3,112151,12000.30,12647.329）\n",
    "ミルク（55,73498,5796.27,7380.377）\n",
    "食料品（3,92780,7951.28,9503.163）\n",
    "FROZEN（25,60869,3071.93,44854.673）\n",
    "DETERGENTS_PAPER（3、40827、2881.49、4767.854）\n",
    "デリカテッセン（DELICATESSEN）（3,47943,1524.87,2820.106）\n",
    "\n",
    "地域の頻度\n",
    "リスボン77\n",
    "ポルト47\n",
    "その他の地域316\n",
    "合計440\n",
    "\n",
    "チャネル周波数\n",
    "ホレカ298\n",
    "小売142\n",
    "合計440  \n",
    "\n",
    "関連論文：\n",
    "Cardoso、Margarida G.M.S. （2013）。 論理弁別モデル - Luiz MoutinhoとKun-Huang Huarngによって編集されたマーケティングとマネジメントの定量モデリングにおける第8章。 World Scientific。 p。 223-253。 ISBN 978-9814407717\n",
    "Jean-Patrick Baudry、Margarida Cardoso、Gilles Celeux、Maria Jos Amorim、Ana Sousa Ferreira（2012）が含まれます。 外部定性変数によるモデルベースのクラスタリングの選択を強化する。 研究報告書N°8124、2012年10月、プロジェクトチームSELECT。 INRIA Saclay - Žle-de-France、プロジェクション、Université©Paris-Sud 11\n",
    "引用のリクエスト：\n",
    "データセットは、参照されるより大きなデータベースから生成されます。\n",
    "Abreu、N.（2011）。 アナリシスは、あなたの組織のプロモーションを再開するために必要です。 Mestrado em Marketing、ISCTE-IUL、リスボン"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"サンプルの数：{} 　特徴量の数：{}\\n\".format((*data.shape)))\n",
    "print(\"各特徴量の型\\n\",data.dtypes,\"\\n\")\n",
    "print(\"統計量の要約\")\n",
    "display(data.describe())\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = sns.FacetGrid(data, hue=\"Region\",aspect=4)\n",
    "fig.map(sns.kdeplot,'Channel',shade= True)\n",
    "z = data['Region'].max()\n",
    "fig.set(xlim=(0,z))\n",
    "fig.add_legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【課題】不要な特徴量を消す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Horeca = Hotel/Restaurant/Caféの総称  \n",
    "Retail = 小売り"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_original = data.copy()\n",
    "\n",
    "#dataからChannelとRegionの2列を削除するコードを記述\n",
    "data = data.drop(['Channel','Region'],axis=1)\n",
    "\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ※scatter matrixを描く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.scatter_matrix(data, alpha = 0.3, figsize = (14,8), diagonal = 'kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【課題】Feature Scalingの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "data = np.log(data)\n",
    "data_fs = pd.DataFrame(sc.fit_transform(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【課題】なぜFeature Scalingが必要なのか記述せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え： 単位（Scale）の違う特徴量、例えば身長と体重などを合計して比べようとしても単位が違うために正確に比べられない。そこでFeature Scalingを行い、各特徴量を同じ単位にスケーリングする。  \n",
    "またFeature Scaling前後のFeature Scalingを見ても、Feature Scaling後の図ではデータの分布が正規分布に近くなっている(分散が大きくなった)ためにより正確な計算が可能になると思われる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.scatter_matrix(data_fs, alpha = 0.3, figsize = (14,8), diagonal = 'kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【課題】PCAの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=None)\n",
    "pca.fit(data_fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ev_ratio = pca.explained_variance_ratio_\n",
    "ev_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ev_sum = ev_ratio[0]+ev_ratio[1]\n",
    "ev_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ev_ratio = pca.explained_variance_ratio_\n",
    "ev_ratio = np.hstack([0,ev_ratio.cumsum()])\n",
    "plt.plot(ev_ratio)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_pca = data[['Fresh','Milk']]\n",
    "data_pca.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【課題】なぜPCAを行うのか記述せよ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え：PCAで相関関係にある特徴量同士を任意の数にまとめる事により、特徴量の個数を減らす事が出来る。それにより次元の呪い（過学習）対策や、計算量の削減に繋がるから。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【課題】クラスタリングとは"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え： 　\n",
    "- いつ何に使うのか  \n",
    "教師なし学習で、どういうグループに分割できるかがわからないときにひとまずクラスタリングしてみることによって、サンプルデータ全体を何となくクラスタ（集団）に分けられるよ。という手法。\n",
    "\n",
    "\n",
    "- 階層的手法と非階層的手法との違い  \n",
    "階層的手法とは、個体間の類似度あるいは非類似度（距離）に基づいて、最も似ている個体から順に集めてクラスターを作っていく方法である。  \n",
    "非階層的手法とは、異なる性質のものが混ざり合った集団から互いに似た性質を持つものを集めクラスターを作る方法の１つである。階層的手法とは異なり階層的な構造ではなく、いくつのクラスタに分けるかは前もって分析者が決める必要がある。サンプル数が多いビッグデータを分析する時に適している。\n",
    "\n",
    "\n",
    "- クラス分類（classification）との比較  \n",
    "辞書で調べるとclassは「分野、類、部類、種類」と出てくる。対してclusterは「群れ、集団、一団」と出てくる。  \n",
    "この直訳から解釈するに、クラス分類は対象（正解データ）になる分類を見つけるというニュアンス。クラスタリングはデータそれぞれを集団に分けるというニュアンス。  \n",
    "\n",
    "クラス分類は事前に決まっている分類を新たに入力されたデータに正確に付与する手法。  \n",
    "クラスタリングは、与えられたデータの中だけで集まりを見つける手法。似たものの集合を出力する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【課題】樹状図を描く"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "data_pca.plot(0,1,kind='scatter',ax=ax)\n",
    "for k, v in data_pca.iterrows():\n",
    "    ax.annotate(k,xy=(v[0],v[1]),size=10)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "row_clusters = linkage(pdist(data_pca, metric='euclidean'), method='ward')\n",
    "row_dender = dendrogram(row_clusters)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【課題】k-meansにおけるkの値の検討"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# エルボー法\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "SSE = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(n_clusters=i, init='random', n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "    km.fit(data_pca)\n",
    "    SSE.append(km.inertia_)\n",
    "\n",
    "plt.plot(range(1,11), SSE, marker='o')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('SSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# シルエット分析\n",
    "\n",
    "for i in range(2,6):\n",
    "    km = KMeans(n_clusters=i, init='random', n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "    y_km = km.fit_predict(data_pca)\n",
    "    from matplotlib import cm\n",
    "    from sklearn.metrics import silhouette_samples\n",
    "    import numpy as np\n",
    "    cluster_labels = np.unique(y_km)\n",
    "    n_clusters = cluster_labels.shape[0]\n",
    "    silhouette_vals = silhouette_samples(data_pca, y_km, metric='euclidean')\n",
    "    y_ax_lower, y_ax_upper = 0, 0\n",
    "    yticks = []\n",
    "\n",
    "for i, c in enumerate(cluster_labels):\n",
    "    c_silhouette_vals = silhouette_vals[y_km == c]\n",
    "    c_silhouette_vals.sort()\n",
    "    y_ax_upper += len(c_silhouette_vals)\n",
    "    color = cm.jet(i / n_clusters)\n",
    "    plt.barh(range(y_ax_lower, y_ax_upper), c_silhouette_vals, height=1.0, edgecolor='none', color=color)\n",
    "    yticks.append((y_ax_lower + y_ax_upper) / 2)\n",
    "    y_ax_lower += len(c_silhouette_vals)\n",
    "\n",
    "    silhouette_avg = np.mean(silhouette_vals)\n",
    "    plt.axvline(silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    plt.yticks(yticks, cluster_labels + 1)\n",
    "    plt.ylabel('Cluster')\n",
    "    plt.xlabel('Silhouette coefficient')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k=4  \n",
    "理由：クラスタ数を決める際の判断基準である「樹上図」、「エルボー法」、「シルエット分析」を行い、それぞれの結果(主に樹上図、エルボー法)からk=4にするのが良いと結論付けた。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【課題】k-meansの実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "km = KMeans(n_clusters=3, init='random', n_init=10, max_iter=300, tol=1e-04, random_state=0)\n",
    "y_km = km.fit_predict(data_pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【課題】クラスタリングの可視化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(data_pca['Fresh'],data_pca['Milk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 【課題】既知のグループとの比較"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Channel(卸先)　 = 　ホレカ298 小売142  \n",
    "Region(地域) =　  リスボン77 ポルト47 その他の地域316"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.scatter(data_pca.iloc[:,0],data_pca.iloc[:,1],c=y_km ,marker='o', s=20,cmap='plasma')\n",
    "plt.title('y_km')\n",
    "plt.xlabel('Fresh')\n",
    "plt.ylabel('Milk')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(data_pca.iloc[:,0],data_pca.iloc[:,1],c=data_original['Channel'] ,marker='o', s=20,cmap='plasma')\n",
    "plt.title('Channel')\n",
    "plt.xlabel('Fresh')\n",
    "plt.ylabel('Milk')\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(data_pca.iloc[:,0],data_pca.iloc[:,1],c=data_original['Region'] ,marker='o', s=20,cmap='plasma')\n",
    "plt.title('Region')\n",
    "plt.xlabel('Fresh')\n",
    "plt.ylabel('Milk')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答え：比較した結果、今回のクラスタリングで分けられたクラスタはRegionとFreshとの相関はないと思われる。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X = data_pca.iloc[:,0] , y = data_pca.iloc[:,1]  c=y_km で分類されたクラスタ毎に色分けされる。\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "plt.scatter(data_pca.iloc[:,0],data_pca.iloc[:,1],c=y_km ,marker='o', s=20,cmap='plasma')\n",
    "plt.title('plot_1')\n",
    "plt.xlabel('Fresh')\n",
    "plt.ylabel('Milk')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_original['y_km'] = y_km\n",
    "data_original['total'] = data_original['Fresh'] + data_original['Milk'] + data_original['Grocery'] + data_original['Frozen'] + data_original['Detergents_Paper']\n",
    "data_original = data_original.drop(['Channel','Region'],axis=1)\n",
    "display(data_original.groupby('y_km').mean())\n",
    "#display(data_original.groupby('y_km').median())\n",
    "#display(data_original.groupby('y_km').max())\n",
    "#display(data_original.groupby('y_km').min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 【課題】クラスタの説明"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "答え：上記の図は３つのクラスタ（集団）に分かれている。これはコンピュータが与えられたデータを、そのデータが持つ或る特徴を基にクラスタリング(データをクラスタに分ける事)したものである。  \n",
    "コンピュータはデータを特徴ごとにクラスタリングしてはくれるが、データが持つどんな特徴によってクラスタリングしたかは教えてくれない。なのでコンピュータがクラスタリングした基準は、人間がその結果を基に推察する必要がある。   \n",
    "\n",
    "※今回の結果（上記図　plot_1）を基にしたクラスタリング基準の推察は下記参照。  （y_kmの値=クラスタ毎に分けられた各特徴の平均の表も参照）\n",
    "\n",
    "#### 赤色（図 y_km１）のクラスタ →　FreshとMilk両方の年間消費量が高いクラスタ。  \n",
    "#### 青色（図 y_km0）のクラスタ　→ Freshは少ないがMilkの年間消費量が高いクラスタ。  \n",
    "#### 黄色（図 y_km２）のクラスタ　→ Milkは少ないがFreshの年間消費量が高いクラスタ。  \n",
    "\n",
    "  \n",
    "    \n",
    "と推察出来る。  なのでこのクラスタリングを基にChannel(卸先)やRegion(地域)以外での、クラスタ毎での消費量が多い商品の種類を増やすなどの消費拡大戦略を考えてみると良いのではないでしょうか？"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
